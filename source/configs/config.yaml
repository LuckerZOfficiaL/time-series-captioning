general:
  random_seed: 42

data:
  dataset_names: [air quality, border crossing, crime, demography]
  request_augmentations: 0 # how many times to rephrase the original prompt request?
  n_samples: 5 # how many window samples to extract per dataset? i.e. how many time series to sample? Note that this is just the number of real samples and not the number of captions because each model in used_models generates its version of the caption. So, the number of captions generated will be n_samples*len(used_models).
  save_top_k: 0 # save the top k best captions based on the ranking, if it's 0 or negative, don't do top-k. If top-k is on, caption ranking is invoked

model:
  all_models: [Google Gemini-2.0-Flash, OpenAI GPT-4o, Anthropic Claude-3.5, GPT-4o, Claude-3.5-Haiku, Gemini-1.5-Flash, Gemini-1.5-Pro, DeepSeek-R1-FW] # available model choices, the first two are from official APIs
  used_models: [OpenAI GPT-4o, Anthropic Claude-3.5, Google Gemini-2.0-Flash] # models to use for generating captions
  refinement_model: Google Gemini-2.0-Flash
  checking_model: Google Gemini-2.0-Flash
  ranking_model: OpenAI GPT-4o # the model used to rank the captions
  extraction_model: Google Gemini-2.0-Flash
  filter_model: Google Gemini-2.0-Flash #"OpenAI GPT-4o" #"Gemini-2.0-Flash"
  embedding_model: all-MiniLM-L6-v2 # the sentence transformer model used for embeddings in RAG
  remove_common_sense_model: Google Gemini-2.0-Flash #"OpenAI GPT-4o" #"Gemini-2.0-Flash"

refinement:
  refinement_types: [add facts, change style, enrich language, factual checking]
  refinement_type: factual checking
  desired_style: casual # matters only if refinement_type == change style
  refinement_target: fake #raw, rag, refined/add facts ----- What captions to refine?
  batch_size: 5 # how many facts to present to the LLM in each prompt for removing common sense and checking facts?
  factual_correction_method: llm # fill in the gap
  skip_numeric: True # whether to skip numeric statements in factual checking, so that they get all preserved

factcheck:
  start_from_files: False # If False, the benchmark starts by applying our fake detection method and save the txt files. If True, it starts by reading the txt files instead of re-generating the facts

path: # paths that have "folder" in its name are folder paths, otherwise they are file paths
  caption_folder_path: /home/ubuntu/thesis/data/samples/captions
  ts_folder_path: /home/ubuntu/thesis/data/samples/time series
  plot_folder_path: /home/ubuntu/thesis/data/samples/plots
  refined_captions_folder_path: /home/ubuntu/thesis/data/samples/captions/refined
  extracted_facts_folder_path: /home/ubuntu/thesis/data/samples/captions/extracted facts
  filtered_facts_folder_path: /home/ubuntu/thesis/data/samples/captions/filtered facts
  fact_bank_folder_path: /home/ubuntu/thesis/data/fact bank
  all_facts_path: /home/ubuntu/thesis/data/fact bank/all_facts.txt
  all_facts_no_common_sense_path: /home/ubuntu/thesis/data/fact bank/all_facts_no_common_sense.txt
  all_facts_by_period_folder_path: /home/ubuntu/thesis/data/fact bank/by period #/{BIN_YEARS}/all_facts_by_{BIN_YEARS}years.json
  prototypes_path: /home/ubuntu/thesis/model/prototype_words.txt

mobtep:
  output_text: True # if False, it outputs the fused and aligned embedding
  tcn_emb_size: 128
  use_linear_proj: False

nlp:
 synonym_similarity_thresh: 0.7

plot:
  height: null
  width: null

rag:
  use_rag: False # whether to apply RAG on caption generation, it will only retrieve the facts that are temporally relevant to the prompt request.
  rag_top_k: 5 # how many top-relevant facts to retrieve from the bank

bank:
  save_pca: True # whether to save the 2D PCA of fact embeddings as a jpeg file
  bin_years: 10 # how long is a single period in years?

  